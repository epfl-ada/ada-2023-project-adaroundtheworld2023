{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661f890a50442989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Similarities \n",
    "\n",
    "As we now have embeddings, it's time to save the similarities to JSONs. The goal is to take the embeddings we generated in the previous steps and by using dot product calculate cosine similarity between all the movies within every decade. We do it by using two different approaches:\n",
    "\n",
    "1. **embedding approach**: we used e5-large model to generate normalized embeddings of size 1024 (see [embeddings.ipynb](embeddings.ipynb));\n",
    "2. **genre approach**: we picked genres and themes and let bart-large model to predict the probabilities of belonging to a genre/theme, which we turned into vector representations and normalized into unit vectors (see [embeddings.ipynb](embeddings.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ed3e6d43e682e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T09:39:35.423168Z",
     "start_time": "2023-12-15T09:39:35.190802Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from scripts.helpers import get_embeddings_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc974e25cbcbc810",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T09:39:36.331369Z",
     "start_time": "2023-12-15T09:39:36.306606Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1170d40ddd854",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "Since we're already embedded the movies using Google Colab and downloaded them (see README), we can simply read the json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a8fd5913a5624",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decades = np.arange(1910, 2020, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea69c2cd89f487d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T10:41:08.224089Z",
     "start_time": "2023-11-28T10:41:08.221117Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for decade in decades:\n",
    "    embeddings = get_embeddings_from_json(decade, approach='embedding')\n",
    "    \n",
    "    # generate all possible combinations for dot product \n",
    "    combinations = list(itertools.combinations(embeddings.keys(), 2))\n",
    "    \n",
    "    similarities = {}\n",
    "    \n",
    "    for movie_1, movie_2 in tqdm(combinations):\n",
    "        similarity = embeddings[movie_1] @ embeddings[movie_2]\n",
    "        similarities['-'.join([str(movie_1), str(movie_2)])] = similarity\n",
    "    \n",
    "    filepath = os.path.join(root_path, 'data', 'similarities', 'embedding', f'similarities_{decade}s.json')\n",
    "    with open(filepath, \"w\") as outfile: \n",
    "        json.dump(similarities, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a635ccb3a921f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Genres\n",
    "\n",
    "We didn't merge for loops for the two approaches, because the first one is computationally much more heavy, so we kept it separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c176d337af67b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T09:49:19.358513Z",
     "start_time": "2023-12-15T09:45:41.262345Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6786it [00:00, 279581.81it/s]\n",
      "85078it [00:00, 299702.35it/s]\n",
      "862641it [00:02, 323413.80it/s]\n",
      "1088550it [00:03, 288493.19it/s]\n",
      "1604736it [00:04, 348812.13it/s]\n",
      "1313010it [00:03, 361166.52it/s]\n",
      "1554966it [00:04, 356145.14it/s]\n",
      "3296028it [00:09, 344640.35it/s]\n",
      "5700376it [00:23, 241693.26it/s]\n",
      "19453203it [01:10, 275713.85it/s]\n",
      "1461195it [00:04, 307364.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for decade in decades:\n",
    "\n",
    "    embeddings = get_embeddings_from_json(decade, approach='custom_genres')\n",
    "    \n",
    "    similarities = {}\n",
    "    \n",
    "    for movie_1, movie_2 in tqdm(itertools.combinations(embeddings.keys(), 2)):\n",
    "        similarity = embeddings[movie_1] @ embeddings[movie_2]\n",
    "        similarities['-'.join([str(movie_1), str(movie_2)])] = similarity\n",
    "    \n",
    "    filepath = os.path.join(root_path, 'data', 'similarities', 'custom_genres', f'similarities_{decade}s.json')\n",
    "    with open(filepath, \"w\") as outfile: \n",
    "        json.dump(similarities, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
